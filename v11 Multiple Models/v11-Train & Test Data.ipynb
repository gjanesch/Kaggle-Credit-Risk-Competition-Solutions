{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>This file is for processing the <TT>application_train</TT> and <TT>application_test</TT> data files.</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import v10_common as com\n",
    "\n",
    "import feather\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as sstats\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further work:\n",
    "# - try making REGION_RATING_CLIENT a three-level factor\n",
    "# - NAME_FAMILY_STATUS: \"Unknown\" should be NA, try dividing rest into \"Married\" and \"Not Married\"\n",
    "# - try keeping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_feather(com.DATA_FILE_FOLDER + \"application_train.feather\")\n",
    "test = pd.read_feather(com.DATA_FILE_FOLDER + \"application_test.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's a couple things we need to hang on to in order to keep things separate\n",
    "target = train[\"TARGET\"]\n",
    "train_IDs = train[\"SK_ID_CURR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(\"TARGET\", inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Engineering of the train and test data.  For consistency's sake, the train and test data are combined into a single dataframe, the needed operations are run on it, and then they are split apart. </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([train,test], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column for total number of docs provided\n",
    "document_columns = [\"FLAG_DOCUMENT_\" + str(x) for x in range(2,22)]\n",
    "all_data[\"NUM_DOCUMENTS_PROVIDED\"] = all_data[document_columns].sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplify ORGANIZATION_TYPE a bit; see if this helps anything\n",
    "industry_types = [\"Industry: type \" + str(x) for x in range(1,14)]\n",
    "trade_types = [\"Trade: type \" + str(x) for x in range(1,8)]\n",
    "transport_types = [\"Transport: type \" + str(x) for x in [1,2,3,4]]\n",
    "useless_ORGANIZATION_TYPE = [\"Culture\",\"Electricity\",\"Emergency\",\"Hotel\",\"Insurance\",\"Legal Services\",\n",
    "                             \"Mobile\", \"Religion\", \"Telecom\"]\n",
    "\n",
    "all_data[\"ORGANIZATION_TYPE\"].replace(industry_types, \"Industry\", inplace = True)\n",
    "all_data[\"ORGANIZATION_TYPE\"].replace(trade_types, \"Trade\", inplace = True)\n",
    "all_data[\"ORGANIZATION_TYPE\"].replace(transport_types, \"Transport\", inplace = True)\n",
    "all_data[\"ORGANIZATION_TYPE\"].replace(\"XNA\", np.nan, inplace = True)\n",
    "all_data[\"ORGANIZATION_TYPE\"].replace(useless_ORGANIZATION_TYPE, np.nan, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform some original columns\n",
    "all_data.loc[all_data[\"CODE_GENDER\"] == \"XNA\", \"CODE_GENDER\"] = \"F\"\n",
    "all_data.loc[all_data[\"DAYS_EMPLOYED\"] == 365243, \"DAYS_EMPLOYED\"] = np.NaN\n",
    "\n",
    "name_type_suite = all_data[\"NAME_TYPE_SUITE\"]\n",
    "all_data.loc[(name_type_suite != \"Unaccompanied\") & (~name_type_suite.isnull()), \"NAME_TYPE_SUITE\"] = \"Accompanied\"\n",
    "\n",
    "# Remove useless NAME_INCOME_TYPE values\n",
    "useless_NAME_INCOME_TYPE = [\"Businessman\",\"Pensioner\",\"Student\",\"Unemployed\"]\n",
    "all_data[\"NAME_INCOME_TYPE\"].replace(useless_NAME_INCOME_TYPE, np.nan, inplace = True)\n",
    "\n",
    "# Remove useless OCCUPATION_TYPE values (hang on to 'Private service staff' since LR might find it useful)\n",
    "useless_OCCUPATION_TYPE = [\"Cooking staff\", \"HR staff\", \"Private service staff\", \"Realty agents\"]\n",
    "all_data[\"OCCUPATION_TYPE\"].replace(useless_OCCUPATION_TYPE, np.nan, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All-new features\n",
    "all_data[\"CREDIT_INCOME_RATIO\"] = all_data[\"AMT_CREDIT\"] / all_data[\"AMT_INCOME_TOTAL\"]\n",
    "all_data[\"INCOME_PER_HEAD\"] = all_data[\"AMT_INCOME_TOTAL\"] / all_data[\"CNT_FAM_MEMBERS\"]\n",
    "all_data[\"ANNUITY_INCOME_RATIO\"] = all_data[\"AMT_ANNUITY\"] / all_data[\"AMT_INCOME_TOTAL\"]\n",
    "all_data[\"ANNUITY_CREDIT_RATIO\"] = all_data[\"AMT_ANNUITY\"] / all_data[\"AMT_CREDIT\"]\n",
    "all_data[\"GOODS_INCOME_RATIO\"] = all_data[\"AMT_GOODS_PRICE\"] / all_data[\"AMT_INCOME_TOTAL\"]\n",
    "all_data[\"GOODS_CREDIT_RATIO\"] = all_data[\"AMT_GOODS_PRICE\"] / all_data[\"AMT_CREDIT\"]\n",
    "all_data[\"FRAC_DAYS_EMPLOYED\"] = all_data[\"DAYS_EMPLOYED\"] / all_data[\"DAYS_BIRTH\"]\n",
    "all_data[\"FRAC_CHILDREN\"] = all_data[\"CNT_CHILDREN\"] / all_data[\"CNT_FAM_MEMBERS\"]\n",
    "\n",
    "ext_sources = all_data[[\"EXT_SOURCE_1\", \"EXT_SOURCE_2\", \"EXT_SOURCE_3\"]]\n",
    "for func_name in [\"SUM\",\"MEAN\",\"MIN\",\"MAX\"]:\n",
    "    all_data[\"EXT_SOURCE_\" + func_name] = eval(f\"np.{func_name.lower()}\")(ext_sources, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [all_data[\"CNT_CHILDREN\"] == 0, all_data[\"CNT_CHILDREN\"] == 1, all_data[\"CNT_CHILDREN\"] >= 2]\n",
    "choices = [\"0\",\"1\",\"2+\"]\n",
    "all_data[\"CNT_CHILDREN\"] = np.select(conditions, choices)\n",
    "\n",
    "all_data[\"AMT_INCOME_TOTAL\"] = np.log10(all_data[\"AMT_INCOME_TOTAL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete columns unimportant to both the logistic regression and LGBM\n",
    "unimportant_columns = [\"FLAG_DOCUMENT_4\", \"FLAG_DOCUMENT_7\", \"FLAG_DOCUMENT_10\", \"FLAG_DOCUMENT_12\",\n",
    "                       \"FLAG_DOCUMENT_21\", \"FONDKAPREMONT_MODE\", \"FLAG_MOBIL\", \"FLAG_EMP_PHONE\",\n",
    "                       \"AMT_REQ_CREDIT_BUREAU_HOUR\", \"HOUSETYPE_MODE\", \"EMERGENCYSTATE_MODE\"]\n",
    "all_data.drop(unimportant_columns, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn all of the remaining string columns into onehot data\n",
    "\n",
    "object_cols = [col for col in all_data.columns if all_data[col].dtype == \"O\"]\n",
    "onehot_dfs = []\n",
    "for oc in object_cols:\n",
    "    onehot_dfs.append(com.string_col_to_onehot(all_data, oc))\n",
    "all_data.drop(object_cols, inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([all_data] + onehot_dfs, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Logistic regression predictions.  Several columns are stripped out due to multicollinearity issues (the threshold point for this is a correlation of greater than 0.75 between two variables).  A few other columns are transformed so that they have higher correlations with the TARGET variable (which seems to help the models along); since the ranking order of the points stays the same in these circumstances, the gradient boosting shouldn't be particularly affected. </h4>\n",
    "\n",
    "<h4> LAST AUC VALUE: 0.7160 </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High Correlation Pairs:\n",
    "    \n",
    "NOT CHECKED YET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load objects needed for logistic regression\n",
    "target_df = pd.read_feather(\"target.feather\")\n",
    "\n",
    "all_data_poly = {\"DAYS_BIRTH\":2,\"DAYS_REGISTRATION\":2,\"DAYS_ID_PUBLISH\":2}\n",
    "\n",
    "high_cor_columns = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7172525981704014\n",
      "0.7159998776126388\n",
      "0.715340976041085\n",
      "0.7128817480973778\n",
      "0.7149935648610101\n",
      "Avg AUC: 0.7152937529565027\n"
     ]
    }
   ],
   "source": [
    "# Make logistic regression predictions\n",
    "test_aucs = []\n",
    "for _ in range(5):\n",
    "    pred, auc = com.log_regress_other_files(com.add_polynomial_terms(all_data.copy(), all_data_poly),\n",
    "                                            target_df,\n",
    "                                            high_cor_columns)\n",
    "    test_aucs.append(auc)\n",
    "    print(auc)\n",
    "print(\"Avg AUC: \" + str(np.mean(test_aucs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"TRAIN_DATA_LR_PREDS\"] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.to_feather(\"all_data.feather\")\n",
    "pd.DataFrame({\"SK_ID_CURR\":train_IDs, \"TARGET\":target}).to_feather(\"target.feather\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
