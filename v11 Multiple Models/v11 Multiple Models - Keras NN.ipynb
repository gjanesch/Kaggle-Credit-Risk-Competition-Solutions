{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution attempt 11, part 1: neural network with improvements\n",
    "# Final submission: \n",
    "# Submission score: 0.762\n",
    "\n",
    "# This neural network is a revival of v05 code in order to act as part of a multi-model prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import v11_common as com\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import feather\n",
    "import re\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, AlphaDropout\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_feather(\"all_data.feather\")\n",
    "target_df = pd.read_feather(\"target.feather\")\n",
    "\n",
    "train_IDs = all_data[:len(target_df)][\"SK_ID_CURR\"]\n",
    "test_IDs = all_data.loc[len(target_df):,\"SK_ID_CURR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.drop([c for c in all_data.columns if re.search(\"^ORGANIZATION_\", c)], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_df = pd.read_feather(\"bureau_sub.feather\").set_index(\"SK_ID_CURR\")\n",
    "prev_app_df = pd.read_feather(\"previous_application_sub.feather\").set_index(\"SK_ID_CURR\")\n",
    "cc_df = pd.read_feather(\"credit_card_sub.feather\").set_index(\"SK_ID_CURR\")\n",
    "install_payment_df = pd.read_feather(\"installments_payments_sub.feather\").set_index(\"SK_ID_CURR\")\n",
    "POS_cash_df = pd.read_feather(\"POS_cash_sub.feather\").set_index(\"SK_ID_CURR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "supplemental_file_df = bureau_df.join(prev_app_df, how = \"outer\")\n",
    "supplemental_file_df = supplemental_file_df.join(cc_df, how = \"outer\")\n",
    "supplemental_file_df = supplemental_file_df.join(install_payment_df, how = \"outer\")\n",
    "supplemental_file_df = supplemental_file_df.join(POS_cash_df, how = \"outer\")\n",
    "supplemental_cols = supplemental_file_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = all_data.join(supplemental_file_df, how = \"left\", on = \"SK_ID_CURR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data[supplemental_cols] = merged_data[supplemental_cols].fillna(0).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.dropna(axis = 1, inplace = True)\n",
    "merged_data.drop(\"SK_ID_CURR\", axis = 1, inplace = True)\n",
    "merged_data.drop([\"PERCENT_AMT_ATM_DRAWINGS_MAX\",\"PERCENT_AMT_ATM_DRAWINGS_AMEAN\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = StandardScaler()\n",
    "sc.fit(merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_orig = merged_data.iloc[:len(target_df),:].copy()\n",
    "test = merged_data.iloc[len(target_df):,:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artificially increase the number of TARGET = 1 cases\n",
    "train = train_orig.copy()\n",
    "train = pd.concat([train, target_df[\"TARGET\"]], axis = 1)\n",
    "train_real = train.loc[target_df[\"TARGET\"] == 1,:]\n",
    "train = pd.concat([train_real,train,train_real])\n",
    "train = train.sample(frac = 1).reset_index(drop = True)\n",
    "target2 = train[\"TARGET\"]\n",
    "train.drop(\"TARGET\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = sc.transform(train)\n",
    "test = sc.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_nn_classifier():\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(units = 350, kernel_initializer = 'uniform', activation = 'elu', input_dim = train.shape[1]))\n",
    "    #classifier.add(BatchNormalization())\n",
    "    classifier.add(Dropout(rate = 0.3))\n",
    "    classifier.add(Dense(units = 200, kernel_initializer = 'uniform', activation = 'elu'))\n",
    "    #classifier.add(BatchNormalization())\n",
    "    classifier.add(Dropout(rate = 0.3))\n",
    "    classifier.add(Dense(units = 150, kernel_initializer = 'uniform', activation = 'elu'))\n",
    "    #classifier.add(BatchNormalization())\n",
    "    classifier.add(Dropout(rate = 0.3))\n",
    "    classifier.add(Dense(units = 85, kernel_initializer = 'uniform', activation = 'elu'))\n",
    "    #classifier.add(BatchNormalization())\n",
    "    classifier.add(Dropout(rate = 0.3))\n",
    "    classifier.add(Dense(units = 40, kernel_initializer = 'uniform', activation = 'elu'))\n",
    "    #classifier.add(BatchNormalization())\n",
    "    classifier.add(Dropout(rate = 0.3))\n",
    "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN Training -- done in 14.997320890426636 sec\n",
      "0.7833562998792846\n",
      "NN Training -- done in 13.88657522201538 sec\n",
      "0.7831617124575226\n",
      "NN Training -- done in 15.36235237121582 sec\n",
      "0.7824977028096484\n",
      "NN Training -- done in 15.411657571792603 sec\n",
      "0.7811878222416998\n",
      "NN Training -- done in 13.577293872833252 sec\n",
      "0.7810829918562088\n",
      "NN Training -- done in 13.650235891342163 sec\n",
      "0.7788608715288631\n",
      "NN Training -- done in 14.8536958694458 sec\n",
      "0.7814507520047171\n",
      "NN Training -- done in 14.752395629882812 sec\n",
      "0.7803443815664097\n",
      "Average AUC: 0.7814928167930443\n"
     ]
    }
   ],
   "source": [
    "#import keras.backend as K\n",
    "# may be able to use the backend to calculate AUC\n",
    "\n",
    "N_ITER = 8\n",
    "\n",
    "val_aucs = []\n",
    "\n",
    "for _ in range(N_ITER):\n",
    "    train_train, train_val, target_train, target_val = train_test_split(train, target2)\n",
    "    \n",
    "    classifier = keras_nn_classifier()\n",
    "    \n",
    "    with com.timer(\"NN Training\"):\n",
    "        classifier.fit(train_train, target_train, batch_size = 5000, epochs = 15, verbose = 0)\n",
    "    \n",
    "    val_predictions = classifier.predict(train_val)\n",
    "    auc = roc_auc_score(target_val, val_predictions)\n",
    "    val_aucs.append(auc)\n",
    "    print(auc)\n",
    "print(f\"Average AUC: {np.mean(val_aucs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC: 0.7966232662574978\n",
      "Test ROC: 0.7796471295328824\n"
     ]
    }
   ],
   "source": [
    "train_predictions = classifier.predict(train_train)\n",
    "test_predictions = classifier.predict(train_val)\n",
    "roc_train = roc_auc_score(target_train, train_predictions)\n",
    "roc_test = roc_auc_score(target_val, test_predictions)\n",
    "print(f\"Train ROC: {roc_train}\\nTest ROC: {roc_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "357161/357161 [==============================] - 4s 12us/step - loss: 0.4900 - acc: 0.7855\n",
      "Epoch 2/15\n",
      "357161/357161 [==============================] - 3s 10us/step - loss: 0.4410 - acc: 0.8045\n",
      "Epoch 3/15\n",
      "357161/357161 [==============================] - 4s 10us/step - loss: 0.4372 - acc: 0.8069\n",
      "Epoch 4/15\n",
      "357161/357161 [==============================] - 4s 10us/step - loss: 0.4352 - acc: 0.8080\n",
      "Epoch 5/15\n",
      "357161/357161 [==============================] - 4s 10us/step - loss: 0.4330 - acc: 0.8085\n",
      "Epoch 6/15\n",
      "357161/357161 [==============================] - 4s 10us/step - loss: 0.4317 - acc: 0.8096\n",
      "Epoch 7/15\n",
      "357161/357161 [==============================] - 4s 10us/step - loss: 0.4301 - acc: 0.8100\n",
      "Epoch 8/15\n",
      "357161/357161 [==============================] - 4s 10us/step - loss: 0.4288 - acc: 0.8108\n",
      "Epoch 9/15\n",
      "357161/357161 [==============================] - 4s 10us/step - loss: 0.4274 - acc: 0.8116\n",
      "Epoch 10/15\n",
      "357161/357161 [==============================] - 4s 10us/step - loss: 0.4260 - acc: 0.8125\n",
      "Epoch 11/15\n",
      "357161/357161 [==============================] - 4s 10us/step - loss: 0.4240 - acc: 0.8138\n",
      "Epoch 12/15\n",
      "357161/357161 [==============================] - 4s 10us/step - loss: 0.4223 - acc: 0.8140\n",
      "Epoch 13/15\n",
      "357161/357161 [==============================] - 4s 10us/step - loss: 0.4209 - acc: 0.8145\n",
      "Epoch 14/15\n",
      "357161/357161 [==============================] - 4s 10us/step - loss: 0.4201 - acc: 0.8154\n",
      "Epoch 15/15\n",
      "357161/357161 [==============================] - 4s 10us/step - loss: 0.4184 - acc: 0.8159\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efeadb8ed30>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2 = keras_nn_classifier()\n",
    "clf2.fit(train, target2, batch_size = 5000, epochs = 15, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_predictions = clf2.predict(test)\n",
    "train_predictions = clf2.predict(train_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\"SK_ID_CURR\":test_IDs, \"TARGET\":np.reshape(data_predictions, newshape = (-1))})\n",
    "train_preds = pd.DataFrame({\"SK_ID_CURR\":train_IDs, \"TARGET\":np.reshape(train_predictions, newshape = (-1))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"v11_predictions_Keras.csv\", index = False)\n",
    "train_preds.to_csv(\"v11_predictions_Keras_train.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
